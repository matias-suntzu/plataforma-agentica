"""
Suite Completa de Testing para Meta Ads Agent V5
=================================================

Valida:
- Router V4: ClasificaciÃ³n correcta (simple/agentic/multi_agent)
- Coordinator: Routing entre agentes (config/performance/recommendation)
- ConfigAgent: Herramientas de configuraciÃ³n
- PerformanceAgent: Herramientas de rendimiento + ANUNCIOS
- RecommendationAgent: Recomendaciones de optimizaciÃ³n
- OrchestratorV5: Flujo completo end-to-end
- Casos edge: Continuaciones, contexto conversacional, "todas las campaÃ±as"

Ejecutar: python test_meta_ads_agent_v5.py
"""

import os
import sys
import json
from datetime import datetime
from typing import List, Dict, Tuple
from dataclasses import dataclass

# Importar componentes del sistema
from langgraph_agent.orchestration.orchestrator_v5 import orchestrator_v5
from langgraph_agent.orchestration.router_v4 import router_v4
from langgraph_agent.agents.coordinator_agent import coordinator
from langgraph_agent.agents.config_agent import config_agent
from langgraph_agent.agents.performance_agent import performance_agent
from langgraph_agent.agents.recommendation_agent import recommendation_agent

from langchain_core.messages import HumanMessage, AIMessage


# ========== CONFIGURACIÃ“N ==========

@dataclass
class TestCase:
    """Caso de prueba con validaciÃ³n"""
    query: str
    expected_router: str = None  # Ahora es opcional
    expected_coordinator: str = None
    expected_agent: str = None
    description: str = ""
    context: List = None
    should_use_tool: str = None


# ========== TEST CASES ==========

# ğŸ”¥ CRÃTICO: Testing de Anuncios (Nueva Funcionalidad)
ANUNCIOS_TEST_CASES = [
    TestCase(
        query="Â¿QuÃ© anuncio tiene el mejor CTR en Costa Blanca?",
        expected_router="agentic",
        expected_coordinator="performance",
        expected_agent="performance",
        should_use_tool="ObtenerAnunciosPorRendimientoInput",
        description="ğŸ”¥ RANKING: Mejor anuncio por mÃ©trica especÃ­fica"
    ),
    TestCase(
        query="Â¿Hay algÃºn anuncio que ha empeorado y que explique el cambio en el CPA?",
        expected_router="agentic",
        expected_coordinator="performance",
        expected_agent="performance",
        should_use_tool="CompararAnunciosInput",
        description="ğŸ”¥ COMPARACIÃ“N TEMPORAL: Identificar anuncios que empeoraron"
    ),
    TestCase(
        query="Dame todos los anuncios de Baqueira",
        expected_router="agentic",
        expected_coordinator="performance",
        expected_agent="performance",
        should_use_tool="ObtenerAnunciosPorRendimientoInput",
        description="ğŸ”¥ LISTADO COMPLETO: Todos los anuncios (limite=100)"
    ),
    TestCase(
        query="Â¿QuÃ© anuncio explica el aumento del CPA?",
        expected_router="agentic",
        expected_coordinator="performance",
        expected_agent="performance",
        should_use_tool="CompararAnunciosInput",
        description="ğŸ”¥ ANÃLISIS: Anuncio que causa cambio en mÃ©trica"
    ),
    TestCase(
        query="TOP 3 anuncios con mejor CPA en Ibiza",
        expected_router="agentic",
        expected_coordinator="performance",
        expected_agent="performance",
        should_use_tool="ObtenerAnunciosPorRendimientoInput",
        description="ğŸ”¥ TOP N: Ranking con mÃ©trica ordenar_por=cpa"
    ),
    TestCase(
        query="Â¿CuÃ¡l anuncio tiene peor rendimiento?",
        expected_router="agentic",
        expected_coordinator="performance",
        expected_agent="performance",
        should_use_tool="ObtenerAnunciosPorRendimientoInput",
        description="ğŸ”¥ PEOR: Ordenamiento inverso"
    ),
]

# Router V4: ClasificaciÃ³n Simple/Agentic/Multi
ROUTER_TEST_CASES = [
    # SIMPLE (FastPath)
    TestCase(
        query="lista todas las campaÃ±as",
        expected_router="simple",
        description="Listado simple sin mÃ©tricas"
    ),
    TestCase(
        query="Â¿cuÃ¡ntas campaÃ±as activas tengo?",
        expected_router="simple",
        description="Conteo simple"
    ),
    
    # AGENTIC (Config)
    TestCase(
        query="Â¿quÃ© presupuesto tiene Baqueira?",
        expected_router="agentic",
        expected_coordinator="config",
        description="ConfiguraciÃ³n especÃ­fica"
    ),
    TestCase(
        query="estrategia de puja de Ibiza",
        expected_router="agentic",
        expected_coordinator="config",
        description="Config tÃ©cnica"
    ),
    
    # AGENTIC (Performance)
    TestCase(
        query="Â¿cuÃ¡nto he gastado en Costa Blanca esta semana?",
        expected_router="agentic",
        expected_coordinator="performance",
        description="MÃ©trica de rendimiento"
    ),
    TestCase(
        query="conversiones de Menorca Ãºltimos 7 dÃ­as",
        expected_router="agentic",
        expected_coordinator="performance",
        description="MÃ©trica especÃ­fica"
    ),
    TestCase(
        query="compara esta semana con la anterior",
        expected_router="agentic",
        expected_coordinator="performance",
        description="ComparaciÃ³n de perÃ­odos"
    ),
    
    # AGENTIC (Recommendation)
    TestCase(
        query="dame recomendaciones para mejorar el CPA de Baqueira",
        expected_router="agentic",
        expected_coordinator="recommendation",
        description="RecomendaciÃ³n especÃ­fica"
    ),
    TestCase(
        query="Â¿deberÃ­a activar Advantage+ en Ibiza?",
        expected_router="agentic",
        expected_coordinator="recommendation",
        description="Consulta sobre optimizaciÃ³n"
    ),
    
    # MULTI_AGENT
    TestCase(
        query="analiza la campaÃ±a de Baqueira",
        expected_router="multi_agent",
        expected_coordinator="multi",
        description="AnÃ¡lisis completo"
    ),
    TestCase(
        query="Â¿cÃ³mo estÃ¡ Costa del Sol?",
        expected_router="multi_agent",
        expected_coordinator="multi",
        description="AnÃ¡lisis ambiguo â†’ multi"
    ),
]

# ğŸ”„ Testing Contextual (Continuaciones)
CONTEXTUAL_TEST_CASES = [
    TestCase(
        query="baqueira",
        expected_router="agentic",
        expected_coordinator="performance",
        description="ğŸ”„ ContinuaciÃ³n: Respuesta a pregunta del bot",
        context=[
            AIMessage(content="Â¿De quÃ© campaÃ±a quieres ver las mÃ©tricas?")
        ]
    ),
    TestCase(
        query="todas",
        expected_router="agentic",
        expected_coordinator="performance",
        description="ğŸ”„ ContinuaciÃ³n: 'todas' en contexto",
        context=[
            AIMessage(content="Â¿QuÃ© campaÃ±a quieres analizar?")
        ]
    ),
    TestCase(
        query="de la de ibiza",
        expected_router="agentic",
        expected_coordinator="config",
        description="ğŸ”„ ContinuaciÃ³n: Referencia implÃ­cita",
        context=[
            HumanMessage(content="necesito el presupuesto"),
            AIMessage(content="Â¿De quÃ© campaÃ±a necesitas el presupuesto?")
        ]
    ),
]

# ConfigAgent: Herramientas de configuraciÃ³n
CONFIG_AGENT_TEST_CASES = [
    TestCase(
        query="lista todas las campaÃ±as activas",
        should_use_tool="ListarCampanasInput",
        description="Listar campaÃ±as"
    ),
    TestCase(
        query="busca la campaÃ±a de Baqueira",
        should_use_tool="BuscarCampanaPorNombreInput",
        description="Buscar por nombre"
    ),
    TestCase(
        query="presupuesto de Costa Blanca",
        should_use_tool="ObtenerPresupuestoInput",
        description="Presupuesto especÃ­fico (mÃ¡s rÃ¡pido)"
    ),
    TestCase(
        query="dame todos los detalles de Menorca",
        should_use_tool="ObtenerDetallesCampanaInput",
        description="Detalles completos"
    ),
]

# PerformanceAgent: MÃ©tricas de rendimiento
PERFORMANCE_AGENT_TEST_CASES = [
    TestCase(
        query="gasto de Baqueira esta semana",
        should_use_tool="ObtenerMetricasCampanaInput",
        description="MÃ©tricas de campaÃ±a"
    ),
    TestCase(
        query="TOP 5 anuncios de Ibiza",
        should_use_tool="ObtenerAnunciosPorRendimientoInput",
        description="Top anuncios"
    ),
    TestCase(
        query="compara Baqueira vs semana pasada",
        should_use_tool="CompararPeriodosInput",
        description="ComparaciÃ³n de perÃ­odos"
    ),
    TestCase(
        query="CPA global de todas las campaÃ±as",
        should_use_tool="ObtenerCPAGlobalInput",
        description="MÃ©tricas globales"
    ),
]

# RecommendationAgent: Optimizaciones
RECOMMENDATION_AGENT_TEST_CASES = [
    TestCase(
        query="recomienda mejoras para Baqueira",
        should_use_tool="ObtenerRecomendacionesInput",
        description="Recomendaciones especÃ­ficas"
    ),
    TestCase(
        query="analiza oportunidades de Advantage+ en todas las campaÃ±as",
        should_use_tool="ObtenerRecomendacionesInput",
        description="AnÃ¡lisis global"
    ),
]

# ğŸš¨ Casos Edge (Queries problemÃ¡ticas)
EDGE_CASES = [
    TestCase(
        query="Â¿CÃ³mo fueron todas las campaÃ±as?",
        expected_router="agentic",
        expected_coordinator="performance",
        should_use_tool="CompararAnunciosGlobalesInput",
        description="ğŸš¨ EDGE: 'todas' sin especificar â†’ NO preguntar"
    ),
    TestCase(
        query="dame todos los anuncios",
        expected_router="agentic",
        expected_coordinator="performance",
        should_use_tool="ObtenerAnunciosPorRendimientoInput",
        description="ğŸš¨ EDGE: 'todos' â†’ limite=100, NO preguntar"
    ),
    TestCase(
        query="Â¿hay anuncios que hayan empeorado?",
        expected_router="agentic",
        expected_coordinator="performance",
        should_use_tool="CompararAnunciosInput",
        description="ğŸš¨ EDGE: Pregunta sin campaÃ±a especÃ­fica"
    ),
]


# ========== FUNCIONES DE TESTING ==========

class TestRunner:
    """Ejecutor de tests con reportes detallados"""
    
    def __init__(self):
        self.results = {
            "total": 0,
            "passed": 0,
            "failed": 0,
            "errors": 0,
            "details": []
        }
        self.start_time = datetime.now()
    
    def run_router_tests(self, test_cases: List[TestCase]) -> None:
        """Testea Router V4"""
        print("\n" + "="*70)
        print("ğŸ”€ TESTING ROUTER V4")
        print("="*70)
        
        for test in test_cases:
            self.results["total"] += 1
            
            try:
                result = router_v4.classify(test.query, messages=test.context)
                
                passed = result.category == test.expected_router
                
                if passed:
                    self.results["passed"] += 1
                    status = "âœ… PASS"
                else:
                    self.results["failed"] += 1
                    status = "âŒ FAIL"
                
                detail = {
                    "test": test.description or test.query[:50],
                    "query": test.query,
                    "expected": test.expected_router,
                    "got": result.category,
                    "confidence": result.confidence,
                    "passed": passed
                }
                
                self.results["details"].append(detail)
                
                print(f"\n{status} | {test.description or test.query[:50]}")
                print(f"   Query: '{test.query}'")
                print(f"   Expected: {test.expected_router} | Got: {result.category} (conf: {result.confidence:.2f})")
                
                if not passed:
                    print(f"   âš ï¸  Reasoning: {result.reasoning}")
            
            except Exception as e:
                self.results["errors"] += 1
                print(f"\nâŒ ERROR | {test.description}")
                print(f"   Exception: {str(e)}")
    
    def run_coordinator_tests(self, test_cases: List[TestCase]) -> None:
        """Testea Coordinator"""
        print("\n" + "="*70)
        print("ğŸ¯ TESTING COORDINATOR")
        print("="*70)
        
        for test in test_cases:
            if not test.expected_coordinator:
                continue
            
            self.results["total"] += 1
            
            try:
                decision = coordinator.route(test.query)
                
                passed = decision.agent == test.expected_coordinator
                
                if passed:
                    self.results["passed"] += 1
                    status = "âœ… PASS"
                else:
                    self.results["failed"] += 1
                    status = "âŒ FAIL"
                
                print(f"\n{status} | {test.description or test.query[:50]}")
                print(f"   Query: '{test.query}'")
                print(f"   Expected: {test.expected_coordinator} | Got: {decision.agent} (conf: {decision.confidence:.2f})")
                
                if not passed:
                    print(f"   âš ï¸  Reasoning: {decision.reasoning}")
            
            except Exception as e:
                self.results["errors"] += 1
                print(f"\nâŒ ERROR | {test.description}")
                print(f"   Exception: {str(e)}")
    
    def run_agent_tool_tests(self, agent, agent_name: str, test_cases: List[TestCase]) -> None:
        """Testea herramientas de un agente especÃ­fico"""
        print("\n" + "="*70)
        print(f"ğŸ¤– TESTING {agent_name.upper()}")
        print("="*70)
        
        for test in test_cases:
            self.results["total"] += 1
            
            try:
                config = {"configurable": {"thread_id": f"test_{agent_name}_{self.results['total']}"}}
                
                result = agent.invoke(
                    {"messages": [HumanMessage(content=test.query)]},
                    config=config
                )
                
                # Verificar si usÃ³ la herramienta esperada
                tool_calls = []
                for msg in result["messages"]:
                    if hasattr(msg, 'tool_calls') and msg.tool_calls:
                        tool_calls.extend([tc.name if hasattr(tc, 'name') else tc.get('name') for tc in msg.tool_calls])
                
                if test.should_use_tool:
                    passed = test.should_use_tool in tool_calls
                else:
                    passed = True  # Si no especifica tool, solo verifica que no haya error
                
                if passed:
                    self.results["passed"] += 1
                    status = "âœ… PASS"
                else:
                    self.results["failed"] += 1
                    status = "âŒ FAIL"
                
                print(f"\n{status} | {test.description or test.query[:50]}")
                print(f"   Query: '{test.query}'")
                if test.should_use_tool:
                    print(f"   Expected Tool: {test.should_use_tool}")
                    print(f"   Tools Used: {tool_calls}")
                
                final_msg = result["messages"][-1]
                if hasattr(final_msg, 'content'):
                    print(f"   Response (preview): {final_msg.content[:100]}...")
            
            except Exception as e:
                self.results["errors"] += 1
                print(f"\nâŒ ERROR | {test.description}")
                print(f"   Exception: {str(e)}")
    
    def run_orchestrator_tests(self, test_cases: List[TestCase]) -> None:
        """Testea Orchestrator V5 end-to-end"""
        print("\n" + "="*70)
        print("ğŸš€ TESTING ORCHESTRATOR V5 (End-to-End)")
        print("="*70)
        
        for test in test_cases:
            self.results["total"] += 1
            
            try:
                result = orchestrator_v5.process_query(test.query)
                
                # Validar workflow type
                if test.expected_router:
                    if test.expected_router == "simple":
                        expected_workflow = "simple"
                    elif test.expected_router == "agentic":
                        if test.expected_coordinator:
                            expected_workflow = f"agentic_{test.expected_coordinator}"
                        else:
                            expected_workflow = "agentic"
                    elif test.expected_router == "multi_agent":
                        expected_workflow = "multi_agent"
                    else:
                        expected_workflow = test.expected_router
                    
                    passed = (
                        result.workflow_type == expected_workflow or
                        result.workflow_type.startswith(expected_workflow.split('_')[0])
                    )
                else:
                    passed = result.content and len(result.content) > 0
                
                if passed:
                    self.results["passed"] += 1
                    status = "âœ… PASS"
                else:
                    self.results["failed"] += 1
                    status = "âŒ FAIL"
                
                print(f"\n{status} | {test.description or test.query[:50]}")
                print(f"   Query: '{test.query}'")
                if test.expected_router:
                    print(f"   Expected Workflow: {expected_workflow}")
                print(f"   Got Workflow: {result.workflow_type}")
                print(f"   Response (preview): {result.content[:150]}...")
            
            except Exception as e:
                self.results["errors"] += 1
                print(f"\nâŒ ERROR | {test.description}")
                print(f"   Exception: {str(e)}")
    
    def print_summary(self) -> None:
        """Imprime resumen final"""
        elapsed = (datetime.now() - self.start_time).total_seconds()
        
        print("\n" + "="*70)
        print("ğŸ“Š RESUMEN DE TESTS")
        print("="*70)
        print(f"\nâ±ï¸  Tiempo total: {elapsed:.2f}s")
        print(f"\nğŸ“‹ Tests ejecutados: {self.results['total']}")
        print(f"   âœ… Passed: {self.results['passed']}")
        print(f"   âŒ Failed: {self.results['failed']}")
        print(f"   âš ï¸  Errors: {self.results['errors']}")
        
        success_rate = (self.results['passed'] / self.results['total'] * 100) if self.results['total'] > 0 else 0
        print(f"\nğŸ¯ Tasa de Ã©xito: {success_rate:.1f}%")
        
        if success_rate == 100:
            print("\nğŸ‰ Â¡TODOS LOS TESTS PASARON!")
        elif success_rate >= 90:
            print("\nâœ… Tests mayormente exitosos")
        elif success_rate >= 70:
            print("\nâš ï¸  Algunos tests fallaron - revisar")
        else:
            print("\nâŒ Muchos tests fallaron - requiere atenciÃ³n urgente")
        
        # Detalles de fallos
        failed_tests = [d for d in self.results["details"] if not d.get("passed", True)]
        if failed_tests:
            print("\n" + "="*70)
            print("âŒ TESTS FALLIDOS:")
            print("="*70)
            for test in failed_tests:
                print(f"\nğŸ”¸ {test['test']}")
                print(f"   Query: {test['query']}")
                print(f"   Expected: {test['expected']} | Got: {test['got']}")
        
        print("\n" + "="*70)


# ========== QUERIES HABITUALES SUGERIDAS ==========

QUERIES_HABITUALES = """
ğŸ“‹ QUERIES HABITUALES QUE USUARIOS REALES HARÃAN:

ğŸ” ExploraciÃ³n General:
- "Â¿QuÃ© campaÃ±as tengo activas?"
- "Dame un resumen de todas mis campaÃ±as"
- "Â¿CÃ³mo van las campaÃ±as en general?"

ğŸ’° ConfiguraciÃ³n:
- "Â¿CuÃ¡l es el presupuesto de Baqueira?"
- "Â¿QuÃ© estrategia de puja tiene Ibiza?"
- "Lista todas las campaÃ±as pausadas"
- "Â¿EstÃ¡ activado Advantage+ en Costa Blanca?"

ğŸ“Š Rendimiento:
- "Â¿CuÃ¡nto he gastado esta semana?"
- "Â¿CuÃ¡ntas conversiones tuve ayer?"
- "Dame el CPA de todas las campaÃ±as"
- "Â¿QuÃ© destino tiene mejor ROI?"
- "Compara esta semana vs la anterior"

ğŸ¯ Anuncios (NUEVA FUNCIONALIDAD):
- "Â¿QuÃ© anuncios tienen mejor CTR?"
- "Â¿Hay algÃºn anuncio que haya empeorado?"
- "Dame todos los anuncios de Baqueira"
- "Â¿QuÃ© anuncio explica el aumento del CPA?"
- "TOP 5 anuncios con mÃ¡s conversiones"
- "Â¿CuÃ¡l es el peor anuncio de Ibiza?"

ğŸ’¡ OptimizaciÃ³n:
- "Â¿QuÃ© puedo optimizar en mis campaÃ±as?"
- "Dame recomendaciones para reducir el CPA"
- "Â¿DeberÃ­a subir el presupuesto de Baqueira?"
- "Â¿Por quÃ© el CPA de Ibiza es tan alto?"

ğŸ“ˆ AnÃ¡lisis Completo:
- "Analiza la campaÃ±a de Baqueira"
- "Â¿CÃ³mo estÃ¡ Costa del Sol en general?"
- "Dame un reporte completo de Menorca"
- "Analiza rendimiento + dame sugerencias de Ibiza"

ğŸ”„ Conversacionales (Continuaciones):
Usuario: "Â¿CÃ³mo estÃ¡n las campaÃ±as?"
Bot: "Â¿De quÃ© campaÃ±a especÃ­fica?"
Usuario: "Baqueira" â† debe continuar sin preguntar de nuevo

Usuario: "Â¿Hay anuncios con mal rendimiento?"
Bot: "Â¿De quÃ© campaÃ±a?"
Usuario: "todas" â† debe analizar TODAS sin preguntar

ğŸš¨ Casos ProblemÃ¡ticos (Edge Cases):
- "dame todos" â† Â¿todos quÃ©? (anuncios/campaÃ±as/destinos)
- "cÃ³mo fue la semana pasada" â† Â¿de quÃ©?
- "mejora esto" â† sin contexto previo
- "quÃ© anuncio" â† pregunta incompleta

ğŸ¯ Queries Multilenguaje:
- "show me all campaigns"
- "Â¿cuÃ¡nto spent en Baqueira?"
- "dame el top 3 ads"
"""


# ========== MAIN ==========

def main():
    """Ejecuta suite completa de tests"""
    print("\n" + "="*70)
    print("ğŸ§ª META ADS AGENT V5 - SUITE COMPLETA DE TESTING")
    print("="*70)
    print(f"â° Inicio: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    runner = TestRunner()
    
    # 1. Router V4
    runner.run_router_tests(ROUTER_TEST_CASES)
    runner.run_router_tests(CONTEXTUAL_TEST_CASES)
    runner.run_router_tests(ANUNCIOS_TEST_CASES)
    runner.run_router_tests(EDGE_CASES)
    
    # 2. Coordinator
    runner.run_coordinator_tests(ROUTER_TEST_CASES)
    runner.run_coordinator_tests(ANUNCIOS_TEST_CASES)
    
    # 3. Agentes Individuales
    runner.run_agent_tool_tests(config_agent, "ConfigAgent", CONFIG_AGENT_TEST_CASES)
    runner.run_agent_tool_tests(performance_agent, "PerformanceAgent", PERFORMANCE_AGENT_TEST_CASES)
    runner.run_agent_tool_tests(performance_agent, "PerformanceAgent (Anuncios)", ANUNCIOS_TEST_CASES)
    runner.run_agent_tool_tests(recommendation_agent, "RecommendationAgent", RECOMMENDATION_AGENT_TEST_CASES)
    
    # 4. Orchestrator End-to-End
    runner.run_orchestrator_tests(ROUTER_TEST_CASES[:5])  # Subset para no saturar
    runner.run_orchestrator_tests(ANUNCIOS_TEST_CASES[:3])
    runner.run_orchestrator_tests(EDGE_CASES)
    
    # 5. Resumen Final
    runner.print_summary()
    
    # 6. Imprimir queries habituales
    print(QUERIES_HABITUALES)
    
    # 7. Guardar reporte
    report_path = f"test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(runner.results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ Reporte guardado en: {report_path}")
    
    # Retornar cÃ³digo de salida
    sys.exit(0 if runner.results["failed"] == 0 and runner.results["errors"] == 0 else 1)


if __name__ == "__main__":
    main()